{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Image_Retrieval.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtxDWQH4__sp",
        "outputId": "a2899ba7-b00a-4a24-9456-85866eee64f2"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Sep 15 15:04:55 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZJUmbq1MAUJ"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "from torch.utils.data import Dataset, Sampler, DataLoader\n",
        "from torch.utils.data.sampler import BatchSampler\n",
        "\n",
        "import os\n",
        "import imageio\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.model_zoo as model_zoo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUbmLh-MLQIS"
      },
      "source": [
        "# init params:\n",
        "# semihard margin 0.25\n",
        "# batchsize 64 (4,4,4)\n",
        "# embedding size 128\n",
        "# optimizer SGD momentum 0.9 lr 0.001 weight_decay 0.0005 each 2 epoch->lr/10\n",
        "\n",
        "# embedding norma\n",
        "\n",
        "# aug:\n",
        "# randomresizecrop\n",
        "# randomhorizontalflip\n",
        "# normalization\n",
        "\n",
        "# throw away cats were img count < 4 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "8ms5yha6ABhm",
        "outputId": "b0e5f186-1af7-4e4a-fd34-c464678865f7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KasonrzABn9"
      },
      "source": [
        "!cp \"/content/gdrive/My Drive/SOP/train.zip\" \"train.zip\"\n",
        "!cp \"/content/gdrive/My Drive/SOP/sop-splitfile.zip\" \"sop-splitfile.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTf33I0YABuI"
      },
      "source": [
        "!unzip train.zip -d './sop-train'\n",
        "!unzip sop-splitfile.zip -d './sop-splitfile'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp9HtwYnfkRO"
      },
      "source": [
        "split_file = pickle.load(open(\"/content/sop-splitfile/SOP_train_valid_split.pickle\", \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "9iAjOAmVft6c",
        "outputId": "2ca64219-fbbe-4a06-915e-26ecd368da3f"
      },
      "source": [
        "split_file['cabinet_final']['train'].keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['paths', 'product_labels', 'category_labels'])"
            ]
          },
          "execution_count": 14,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xcxh5LMAg4Kj"
      },
      "source": [
        "split_file['cabinet_final']['train']['paths'][500:530]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ-dfILTf9Fp"
      },
      "source": [
        "split_file['cabinet_final']['train']['product_labels'][500:530]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "7XoXF1o6f89U",
        "outputId": "445a6188-c73b-4483-99d4-147a1bf954ba"
      },
      "source": [
        "np.unique(split_file['cabinet_final']['valid']['category_labels'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "execution_count": 29,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19_utQ_yN6K5"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "> **Dataloader**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZYc7VTwNjzh"
      },
      "source": [
        "from torchvision.transforms import (\n",
        "    RandomResizedCrop,\n",
        "    RandomHorizontalFlip,\n",
        "    ColorJitter,\n",
        "    ToTensor,\n",
        "    Resize,\n",
        "    CenterCrop,\n",
        "    Compose,\n",
        "    ToPILImage,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPDYIYcKcx9k"
      },
      "source": [
        "def HWC_to_CHW(img):\n",
        "    return np.transpose(img, (2, 0, 1))\n",
        "\n",
        "\n",
        "class TorchvisionNormalize:\n",
        "    def __init__(self, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, img):\n",
        "        imgarr = np.asarray(img)\n",
        "        proc_img = np.empty_like(imgarr, np.float32)\n",
        "\n",
        "        proc_img[..., 0] = (imgarr[..., 0] / 255.0 - self.mean[0]) / self.std[0]\n",
        "        proc_img[..., 1] = (imgarr[..., 1] / 255.0 - self.mean[1]) / self.std[1]\n",
        "        proc_img[..., 2] = (imgarr[..., 2] / 255.0 - self.mean[2]) / self.std[2]\n",
        "\n",
        "        return HWC_to_CHW(proc_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipQL9RQ_Njzo"
      },
      "source": [
        "train_transforms = Compose(\n",
        "    [\n",
        "        ToPILImage(),\n",
        "        RandomResizedCrop(224, scale=(0.25, 1.0)),\n",
        "        RandomHorizontalFlip(),\n",
        "        TorchvisionNormalize(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "val_transforms = Compose(\n",
        "    [\n",
        "        ToPILImage(), \n",
        "        Resize(256),\n",
        "        CenterCrop(224), \n",
        "        TorchvisionNormalize()\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK39nQGPN8fw"
      },
      "source": [
        "def rebuild_path(path):\n",
        "    s = os.path.split(path)\n",
        "    return s[0] + \"/\" + s[0] + \"_\" + s[1]\n",
        "\n",
        "\n",
        "def load_image_paths(split_file, mode=\"train\"):\n",
        "    img_paths_list = [\n",
        "        [rebuild_path(p[25:]) for p in split_file[cat][mode][\"paths\"]]\n",
        "        for cat in split_file\n",
        "    ]\n",
        "    img_paths_list = np.hstack(img_paths_list).ravel()\n",
        "    return img_paths_list\n",
        "\n",
        "\n",
        "def load_image_labels(split_file, mode=\"train\", label_key=\"category_labels\"):\n",
        "    img_labels_list = [split_file[cat][mode][label_key] for cat in split_file]\n",
        "    img_labels_list = np.hstack(img_labels_list).ravel()\n",
        "    return img_labels_list\n",
        "\n",
        "\n",
        "class SOPImageDataset(Dataset):\n",
        "    def __init__(self, split_file_path, sop_root, transforms, mode=\"train\"):\n",
        "        self.split_file = pickle.load(open(split_file_path, \"rb\"))\n",
        "        self.img_paths_list = load_image_paths(self.split_file, mode)\n",
        "        self.img_cat_labels_list = load_image_labels(\n",
        "            self.split_file, mode, \"category_labels\"\n",
        "        )\n",
        "        self.img_prod_labels_list = load_image_labels(\n",
        "            self.split_file, mode, \"product_labels\"\n",
        "        )\n",
        "        self.sop_root = sop_root\n",
        "\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.img_paths_list[idx]\n",
        "        cat_label = np.array(self.img_cat_labels_list[idx])\n",
        "        prod_label = np.array(self.img_prod_labels_list[idx])\n",
        "\n",
        "        img_path = self.sop_root + path\n",
        "        img = np.asarray(imageio.imread(img_path))\n",
        "\n",
        "        if len(img.shape) == 2:\n",
        "            img = np.asarray(Image.fromarray(img).convert(\"RGB\"))\n",
        "\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "        sample = {\n",
        "            \"img\": img,\n",
        "            \"cat_label\": torch.from_numpy(cat_label),\n",
        "            \"prod_label\": prod_label,\n",
        "        }\n",
        "        return sample\n",
        "\n",
        "\n",
        "class ImageRetrievalSampler(BatchSampler):\n",
        "    def __init__(self, data_source, num_samples=4, num_cats=4, num_prods=4):\n",
        "        self.data_source = data_source\n",
        "        self.num_samples = num_samples\n",
        "        self.num_cats = num_cats\n",
        "        self.num_prods = num_prods\n",
        "        self.cat_unique_labels = np.unique(data_source.img_cat_labels_list)\n",
        "        self.prod_unique_labels = np.unique(data_source.img_prod_labels_list)\n",
        "        self.sf = data_source.split_file\n",
        "\n",
        "    def __iter__(self):\n",
        "        cats = np.random.choice(self.cat_unique_labels, self.num_cats, replace=False)\n",
        "\n",
        "        valid_prod, prod_counts = np.unique(\n",
        "            self.data_source.img_prod_labels_list, return_counts=True\n",
        "        )\n",
        "        valid_prod = valid_prod[np.where(prod_counts >= self.num_prods)]\n",
        "        i = 0\n",
        "        while i < len(self):\n",
        "            out = []\n",
        "            for c in cats:\n",
        "                # print('cat: ', c)\n",
        "                folder = list(self.sf.keys())[c]\n",
        "                prod_labels = self.sf[folder][\"train\"][\"product_labels\"]\n",
        "                prods_to_choice = [p for p in prod_labels if p in valid_prod]\n",
        "                prods = np.random.choice(prods_to_choice, self.num_prods, replace=False)\n",
        "                for p in prods:\n",
        "                    idxs = np.where(self.data_source.img_prod_labels_list == p)[0]\n",
        "                    idxs = np.random.choice(idxs, self.num_samples, replace=False)\n",
        "                    out.extend(idxs)\n",
        "            yield out\n",
        "            i += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_source) // (\n",
        "            self.num_samples * self.num_cats * self.num_prods\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp4dfwPzTDk8"
      },
      "source": [
        "dataset = SOPImageDataset(\n",
        "    \"/content/sop-splitfile/SOP_train_valid_split.pickle\",\n",
        "    \"/content/sop-train\",\n",
        "    train_transforms,\n",
        ")\n",
        "sampler = ImageRetrievalSampler(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpfVoWAETvfD"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i, idxs in enumerate(sampler):\n",
        "    for p in idxs:\n",
        "        plt.imshow((dataset[p][\"img\"]).transpose(1, 2, 0))\n",
        "        plt.show()\n",
        "    if i == 0:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMEUSE-mTvXz"
      },
      "source": [
        "split_file = pickle.load(\n",
        "    open(\"/content/sop-splitfile/SOP_train_valid_split.pickle\", \"rb\")\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "lB4Nfgv3Vc6G",
        "outputId": "9f35df44-d3e8-43ca-8d90-1d33eb3f6ede"
      },
      "source": [
        "split_file.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['cabinet_final', 'bicycle_final', 'chair_final', 'sofa_final', 'mug_final', 'stapler_final', 'toaster_final', 'coffee_maker_final', 'table_final', 'fan_final', 'lamp_final', 'kettle_final'])"
            ]
          },
          "execution_count": 40,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb1Bpn_1NtvX"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "> **Model**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGJpmWtUIJmt"
      },
      "source": [
        "resnet50_url = \"https://download.pytorch.org/models/resnet50-19c8e357.pth\"\n",
        "\n",
        "\n",
        "class FixedBatchNorm(nn.BatchNorm2d):\n",
        "    def forward(self, input):\n",
        "        return F.batch_norm(\n",
        "            input,\n",
        "            self.running_mean,\n",
        "            self.running_var,\n",
        "            self.weight,\n",
        "            self.bias,\n",
        "            training=False,\n",
        "            eps=self.eps,\n",
        "        )\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        self.bn1 = FixedBatchNorm(out_channels)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            out_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=stride,\n",
        "            padding=1,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.bn2 = FixedBatchNorm(out_channels)\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            out_channels, out_channels * 4, kernel_size=1, bias=False\n",
        "        )\n",
        "        self.bn3 = FixedBatchNorm(out_channels * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "        else:\n",
        "            identity = x\n",
        "\n",
        "        out = self.relu(out + identity)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, layers=(3, 4, 6, 3)):\n",
        "        super(Net, self).__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False\n",
        "        )\n",
        "        self.bn1 = FixedBatchNorm(self.in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        # self.layer0 = nn.Sequential(self.conv1, self.bn1, self.relu, self.maxpool)\n",
        "        self.layer1 = self._get_block(64, layers[0], stride=1)\n",
        "        self.layer2 = self._get_block(128, layers[1], stride=2)\n",
        "        self.layer3 = self._get_block(256, layers[2], stride=2)\n",
        "        self.layer4 = self._get_block(512, layers[3], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # nn.AvgPool2d(7, stride=1)\n",
        "\n",
        "    def _get_block(self, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_channels != out_channels * 4:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    self.in_channels,\n",
        "                    out_channels * 4,\n",
        "                    kernel_size=1,\n",
        "                    stride=stride,\n",
        "                    bias=False,\n",
        "                ),\n",
        "                FixedBatchNorm(out_channels * 4),\n",
        "            )\n",
        "        layers = [\n",
        "            Bottleneck(\n",
        "                self.in_channels, out_channels, stride=stride, downsample=downsample\n",
        "            )\n",
        "        ]\n",
        "        self.in_channels = out_channels * 4\n",
        "\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(Bottleneck(self.in_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        #         print('avgpool', x.size())\n",
        "        x = x.view(x.size(0), -1)\n",
        "        #         print('flat', x.size())\n",
        "        # x = self.fc(x)\n",
        "        #         print('out', x.size())\n",
        "        return x\n",
        "\n",
        "\n",
        "def resnet50(pretrained=True):\n",
        "\n",
        "    model = Net(layers=(3, 4, 6, 3))\n",
        "    if pretrained:\n",
        "        state_dict = model_zoo.load_url(resnet50_url)\n",
        "        state_dict.pop(\"fc.weight\")\n",
        "        state_dict.pop(\"fc.bias\")\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "class RetrievalNet(Net):\n",
        "    def __init__(self, embedding_size=128):\n",
        "        super(RetrievalNet, self).__init__()\n",
        "\n",
        "        self.embedding_size = embedding_size\n",
        "        self.backbone = resnet50(True)\n",
        "        self.fc = nn.Linear(2048, embedding_size)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.backbone(x)\n",
        "        out = self.fc(out)\n",
        "        out = F.normalize(out, dim=1, p=2)\n",
        "        return out\n",
        "\n",
        "    def trainable_parameters(self):\n",
        "        return (list(self.backbone.parameters()), list(self.fc.parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74x_nA9c5Dol"
      },
      "source": [
        "model = RetrievalNet()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAyAlDWoTHxx"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "> **Loss**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihkU-o1SIPUD"
      },
      "source": [
        "from itertools import combinations, product\n",
        "\n",
        "# np.random.seed(666)\n",
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, embeddings, labels):\n",
        "        triplets = self._get_triplets(embeddings, labels)\n",
        "        ap_dists = F.pairwise_distance(embeddings[triplets[0]], embeddings[triplets[1]])\n",
        "        an_dists = F.pairwise_distance(embeddings[triplets[0]], embeddings[triplets[2]])\n",
        "        loss = F.relu(ap_dists - an_dists + self.margin)\n",
        "        return loss.mean()\n",
        "\n",
        "    def _get_triplets(self, embeddings, labels):\n",
        "        distance_matrix = torch.norm(embeddings.unsqueeze(1) - embeddings, p=2, dim=2)\n",
        "        unique_labels, counts = torch.unique(labels, return_counts=True)\n",
        "        triplets_idxs = [[] for i in range(3)]\n",
        "\n",
        "        for label in unique_labels:\n",
        "            pos_indices = torch.where(labels == label)[0]\n",
        "            negative_indices = torch.where(torch.logical_not(labels == label))[0]\n",
        "            if pos_indices.shape[0] < 2:\n",
        "                continue\n",
        "\n",
        "            anchor_positives = np.array(list(combinations(pos_indices, 2)))\n",
        "            for ap in anchor_positives:\n",
        "                ap_dist = distance_matrix[ap[0], ap[1]]\n",
        "                an_dist = distance_matrix[ap[0], negative_indices]\n",
        "                sh_idxs = self._get_semihard(ap_dist, an_dist, self.margin)\n",
        "\n",
        "                if sh_idxs is not None:\n",
        "                    neg_idxs = negative_indices[sh_idxs].item()\n",
        "                    triplets_idxs[0].append(ap[0])\n",
        "                    triplets_idxs[1].append(ap[1])\n",
        "                    triplets_idxs[2].append(neg_idxs)\n",
        "        return triplets_idxs\n",
        "\n",
        "    def _get_semihard(self, ap, an, margin=0.25):\n",
        "        # np.random.seed(666)\n",
        "        loss = ap + margin - an\n",
        "        semihard = torch.where(loss > 0)[0]\n",
        "        if semihard.nelement() != 0:\n",
        "            idx = np.random.choice(semihard)\n",
        "        else:\n",
        "            idx = None\n",
        "        return idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ6aEW-1IPR6"
      },
      "source": [
        "embeddings = torch.Tensor(np.random.rand(64, 5))\n",
        "labels = torch.Tensor(np.random.choice([1, 2, 3, 4], 64))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "mcCUNJ6fIPM8",
        "outputId": "673263fa-dd27-4d26-b0bd-eb04a95ddaa4"
      },
      "source": [
        "TripletLoss(0.25)(embeddings, labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.1432)"
            ]
          },
          "execution_count": 277,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42xt_hBOd0nc"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Training\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7WjMnGyIPLG"
      },
      "source": [
        "train_dataset = SOPImageDataset(\n",
        "    \"/content/sop-splitfile/SOP_train_valid_split.pickle\",\n",
        "    \"/content/sop-train\",\n",
        "    train_transforms,\n",
        ")\n",
        "sampler = ImageRetrievalSampler(dataset)\n",
        "\n",
        "dataloader_train = DataLoader(train_dataset, batch_sampler=sampler)\n",
        "\n",
        "model = RetrievalNet().cuda()\n",
        "# model.apply(weights_init)\n",
        "device = torch.device(\"cuda:0\")\n",
        "max_step = len(train_dataset) // 64  # * config.epoch_num\n",
        "\n",
        "# param_groups = model.trainable_parameters()\n",
        "\n",
        "# optimizer = PolyOptimizer(\n",
        "#     model.parameters(),\n",
        "#     lr=config.learning_rate,\n",
        "#     weight_decay=config.learning_rate_decay,\n",
        "#     max_step=max_step,\n",
        "# )\n",
        "\n",
        "optimizer = optim.SGD(\n",
        "    model.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-4, nesterov=True\n",
        ")\n",
        "\n",
        "\n",
        "# checkpoint = torch.load(\"/content/learning_state_without_tricks.pth\")\n",
        "# model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "# optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "# epoch = checkpoint[\"epoch\"]\n",
        "# loss = checkpoint[\"train loss\"]\n",
        "# print(\"restored loss: \", loss)\n",
        "\n",
        "writer = SummaryWriter(\"/content/gdrive/My Drive/SOP/runs\")\n",
        "model.cuda()\n",
        "criterion = TripletLoss(0.25)\n",
        "\n",
        "\n",
        "def train(config):\n",
        "    model.train()\n",
        "\n",
        "    for ep in range(10):\n",
        "        print(\"Epoch{}/{}\".format(ep + 1, config.epoch_num))\n",
        "        if ep == 9:\n",
        "            optimizer.param_groups[0]['lr'] /= 10\n",
        "\n",
        "        train_loss = []\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for step, batch in enumerate(dataloader_train):\n",
        "            #             print(image_batch, label_batch)\n",
        "            img = batch[\"img\"].cuda()\n",
        "            label = batch[\"label\"].cuda()\n",
        "            embeddings = model(img)\n",
        "            loss = criterion(embeddings, label)\n",
        "            train_loss.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total += label.size(0)\n",
        "            if step % 50 == 0:\n",
        "                print(\"step:%5d/%5d\" % (step, max_step))  # *(ep+1)\n",
        "                print(\n",
        "                    \"loss:%.4f\" % (np.mean(train_loss)),\n",
        "                    \"lr: %.4f\" % (optimizer.param_groups[0][\"lr\"]),\n",
        "                )\n",
        "            writer.add_scalar('loss/train', np.mean(train_loss), step)\n",
        "            writer.add_scalar('lr/train', optimizer.param_groups[0]['lr'], step)\n",
        "\n",
        "    torch.save({\n",
        "        'epoch': ep,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'train loss': np.mean(train_loss)\n",
        "    }, '/content/gdrive/My Drive/SOP/runs/learning_state.pth')\n",
        "\n",
        "    # torch.save(model.state_dict(), 'resnet50_SOP.pth')\n",
        "    torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgMjgFkElsSa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}